{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"The logistic regression uses a function called sigmoid function for squashing the line at 0 and 1. The formula for sigmoid function is:\n$$h_{\\theta}(x)=\\frac{1}{1+e^{-z}}$$\n\nwhere z is the given by summation of multiplication of all features i.e. $x_{i}$ and their respective weights $\\theta_{i}$ so:\n$$z=\\theta x_{i}=\\theta_{0}+ \\theta_{1} x_{1} +...................+\\theta_{n} x_{n}$$\nThe cost function of logistic regression is given by:\n$$J(\\theta)=-\\frac{1}{m}\\sum_{i=1}^{m}y_{i}log(h_{\\theta}(x_{i}))+(1-y_{i})log(1-h_{\\theta}(x_{i}))$$\n\nNow we train the model by updating each weight to minimize the cost function in each iteration by using derivative of the cost function with respect to the weight :\n$$\\theta_{j}\\gets \\theta_{j}-\\alpha\\frac{\\partial J}{\\partial \\theta_{j}}$$\n\nNow we take cost function equation and replace log values as follows:\n$$log(h_{\\theta}(x_{i}))=log(\\frac{1}{1+e^{-\\theta x_{i}}})$$\nbecause $log\\frac{a}{b}=log(a)- log(b)$\n$$=log1 -log(1+e^{-\\theta x_{i}})$$ \n$log1=0$ so,\n$$=-log(1+e^{-\\theta x_{i}})$$\n\nalso the next log term,\n$$log(1-h_{\\theta}(x_{i}))=log\\left( 1-\\frac{1}{1+e^{-\\theta x_{i}}} \\right)$$\n$$=log\\left( \\frac{e^{-\\theta x_{i}}}{1+e^{-\\theta x_{i}}}\\right)$$\n$$=log(e^{-\\theta x_{i}})-log(1+e^{-\\theta x_{i}})$$\n$$=-\\theta x_{i}-log(1+e^{-\\theta x_{i}})$$\n\nPutting value of both log terms into cost function equation:\n$$J(\\theta)=-\\frac{1}{m}\\sum_{i=1}^{m}-y_{i}log(1+e^{-\\theta x_{i}})+(1-y_{i})(-\\theta x_{i}-log(1+e^{-\\theta x_{i}}))$$\n$$=-\\frac{1}{m}\\sum_{i=1}^{m}y_{i}\\theta x_{i}-(\\theta x_{i}+log(1+e^{-\\theta x_{i}}))$$\n\nwe can write $\\theta x_{i}$ as $log(e^{\\theta x_{i}})$ so,\n$$=-\\frac{1}{m}\\sum_{i=1}^{m}y_{i}\\theta x_{i}-(log(e^{\\theta x_{i}})+log(1+e^{-\\theta x_{i}}))$$\n\nbecause $log(ab) = log(a)+log(b)$ so,\n$$=-\\frac{1}{m}\\sum_{i=1}^{m}y_{i}\\theta x_{i}-log(e^{\\theta x_{i}}(1+e^{-\\theta x_{i}}))$$\nfinally,\n\n$$J(\\theta)=-\\frac{1}{m}\\sum_{i=1}^{m}y_{i}\\theta x_{i}-log(e^{\\theta x_{i}}+1)$$\nNow the derivative of cost function with respect to $\\theta$ : \n$$\\frac{\\partial J}{\\partial \\theta}=-\\frac{1}{m}\\sum_{i=1}^{m}\\frac{\\partial \\left( y_{i}\\theta x_{i}-log(e^{\\theta x_{i}}+1) \\right) }{\\partial \\theta}$$\npartial differentiation of first term is :\n$$\\frac{\\partial y_{i} x_{i}\\theta}{\\partial \\theta}=y_{i} x_{i}$$\npartial diiferentiation of second term is:\n$$\\frac{\\partial log(e^{\\theta x_{i}}+1)}{\\partial \\theta}=\\frac{\\partial log(e^{\\theta x_{i}}+1)}{\\partial (e^{\\theta x_{i}}+1)}\\frac{\\partial (e^{\\theta x_{i}}+1)}{\\partial \\theta x_{i}}\\frac{\\partial \\theta x_{i}}{\\partial \\theta}$$\n$$\\frac{1}{1+e^{\\theta x_{i}}} e^{\\theta x_{i}}x_{i}$$\nputting it back into the equation :\n$$\\frac{\\partial J}{\\partial \\theta}=-\\frac{1}{m}\\sum_{i=1}^{m}\\left[ y_{i} x_{i}-\\frac{x_{i}e^{\\theta x_{i}}}{1+e^{\\theta x_{i}}} \\right]$$\n$$=\\frac{1}{m}\\sum_{i=1}^{m}\\left[\\frac{x_{i}e^{\\theta x_{i}}}{1+e^{\\theta x_{i}}} -y_{i} x_{i}\\right]$$\n$$=\\frac{1}{m}\\sum_{i=1}^{m}\\left[\\frac{x_{i}}{1+e^{-\\theta x_{i}}} -y_{i} x_{i}\\right]$$\n$$=\\frac{1}{m}\\sum_{i=1}^{m}\\left[h_{\\theta}(x) -y_{i}\\right]x_{i}$$","metadata":{}}]}